{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44034d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "eng_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "rus_stop_words = stopwords.words('russian')\n",
    "eng_stop_words = stopwords.words('english')\n",
    "lemmatizer = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87c02c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теги\n",
    "\n",
    "nltk_to_pymorphy_mapping = {\n",
    "    # Nouns\n",
    "    'NN': 'NOUN',       # Singular or mass noun\n",
    "    'NNS': 'NOUN',      # Plural noun\n",
    "    'NNP': 'NOUN',      # Proper noun, singular\n",
    "    'NNPS': 'NOUN',     # Proper noun, plural\n",
    "    \n",
    "    # Verbs\n",
    "    'VB': 'VERB',       # Base form of verb\n",
    "    'VBD': 'VERB',      # Past tense verb\n",
    "    'VBG': 'VERB',      # Gerund or present participle\n",
    "    'VBN': 'VERB',      # Past participle\n",
    "    'VBP': 'VERB',      # Non-3rd person singular present\n",
    "    'VBZ': 'VERB',      # 3rd person singular present\n",
    "    \n",
    "    # Adjectives\n",
    "    'JJ': 'ADJF',       # Adjective\n",
    "    'JJR': 'ADJF',      # Comparative adjective\n",
    "    'JJS': 'ADJF',      # Superlative adjective\n",
    "    \n",
    "    # Adverbs\n",
    "    'RB': 'ADVB',       # Adverb\n",
    "    'RBR': 'ADVB',      # Comparative adverb\n",
    "    'RBS': 'ADVB',      # Superlative adverb\n",
    "    \n",
    "    # Pronouns\n",
    "    'PRP': 'NPRO',      # Personal pronoun\n",
    "    'PRP$': 'NPRO',     # Possessive pronoun\n",
    "    \n",
    "    # Determiners\n",
    "    'DT': 'ADJF',       # Determiner\n",
    "    \n",
    "    # Prepositions\n",
    "    'IN': 'PREP',       # Preposition\n",
    "    \n",
    "    # Conjunctions\n",
    "    'CC': 'CONJ',       # Coordinating conjunction\n",
    "    \n",
    "    # Interjections\n",
    "    'UH': 'INTJ',       # Interjection\n",
    "    \n",
    "    # Numbers\n",
    "    'CD': 'NUMR',       # Cardinal number\n",
    "    \n",
    "    # Symbols\n",
    "    '$': 'UNKN',        # Dollar sign\n",
    "    '#': 'UNKN',        # Pound sign\n",
    "    \n",
    "    # Others\n",
    "    'FW': 'UNKN',       # Foreign word\n",
    "    'SYM': 'UNKN',      # Symbol\n",
    "    'LS': 'UNKN',       # List item marker\n",
    "    'POS': 'UNKN',      # Possessive ending\n",
    "    'RP': 'UNKN',       # Particle\n",
    "    'TO': 'INFN',       # to (infinitive marker)\n",
    "    'EX': 'UNKN',       # Existential there\n",
    "    'WP': 'NPRO',       # Wh-pronoun\n",
    "    'WP$': 'NPRO',      # Possessive wh-pronoun\n",
    "}\n",
    "\n",
    "def map_nltk_to_pymorphy(nltk_pos_tag):\n",
    "    return nltk_to_pymorphy_mapping.get(nltk_pos_tag, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f19bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лемматизация и удаление знаков препинания \n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if text != None:\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^а-яА-Я]', ' ', text)\n",
    "        text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        split_text = text.split(' ')\n",
    "        pos_list = [lemmatizer.parse(word)[0].tag.POS for word in split_text if word not in rus_stop_words]\n",
    "        word_list = [lemmatizer.parse(word)[0].normal_form for word in split_text if word not in rus_stop_words]\n",
    "        word_pos_pairs = []\n",
    "       \n",
    "        for pos, word in zip(pos_list, word_list):\n",
    "            if pos is not None:\n",
    "                word_pos_pairs.append(word + '_' + pos)\n",
    "\n",
    "        return ' '.join(word_pos_pairs)\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "13edbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лемматизация и удаление знаков препинания \n",
    "\n",
    "def eng_lemmatize_text(text):\n",
    "    if text != None:\n",
    "        text = text.lower()\n",
    "       \n",
    "        pattern = r'[^\\w\\s]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        tokens = [eng_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [token for token in tokens if token != 'wa']\n",
    "        tagged_words = nltk.pos_tag(tokens)\n",
    "        tagged_words = [(word, map_nltk_to_pymorphy(tag)) for word, tag in tagged_words if word not in eng_stop_words]\n",
    "        return ' '.join(['{}_{}'.format(word, pos) for word, pos in tagged_words])\n",
    "        \n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8206eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение файлов для формата fast_align\n",
    "\n",
    "with open('/Users/karpovapolina/xml_to_txt/output-eng-rus_2.txt', 'r', encoding='utf-8') as input_file, open('output-eng-rus.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for line in input_file:\n",
    "        if len(line.strip().split('|||')) != 1:\n",
    "            english_text, russian_text = line.strip().split('|||')\n",
    "            english_lemmatized = eng_lemmatize_text(english_text.strip())\n",
    "            russian_lemmatized = lemmatize_text(russian_text.strip())\n",
    "            output_file.write(f\"{english_lemmatized} ||| {russian_lemmatized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ab282a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lines_with_pattern(input_file, output_file, pattern):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            if pattern not in line:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_file = '/Users/karpovapolina/Downloads/output-eng-rus.txt' \n",
    "output_file = 'output-eng-rus_2.txt' \n",
    "pattern = '###' \n",
    "\n",
    "remove_lines_with_pattern(input_file, output_file, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed8a1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/karpovapolina/Downloads/output-rus-eng-lem-words.txt', 'r') as file1:\n",
    "    file1 = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fe5ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление дополнительных знаков препинания \n",
    "\n",
    "import string\n",
    "additional_punctuation = \"@#$%^&*()_-+=[]{};:'\\\"\\\\|<>,./''’”“‘\"\n",
    "all_punctuation = set(string.punctuation + additional_punctuation)\n",
    "\n",
    "def remove_punctuation(input_str):\n",
    "    input_str = input_str.split('\\t')\n",
    "    word_rus = ''.join(char if char not in all_punctuation_except_apostrophe or re.match(r\"(?<![а-яА-ЯёЁ])'|'(?![а-яА-ЯёЁ])\", char) else '' for char in input_str[2])\n",
    "    word_eng = ''.join(char if char not in all_punctuation_except_apostrophe or re.match(r\"(?<![a-zA-Z])'|'(?![a-zA-Z])\", char) else '' for char in input_str[3][:-1])\n",
    "    return input_str[0] + '\\t' + input_str[1] + '\\t' + word_rus + '\\t' + word_eng + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45099d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = []\n",
    "for i in file1:\n",
    "    new_file.append(remove_punctuation(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d602300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка данных от пар слов, которые встречаются менее трёх раз\n",
    "\n",
    "final_file = []\n",
    "for i in new_file:\n",
    "    i = i.split('\\t')\n",
    "    if int(i[0]) >= 3 and i[2] != '' and i[3][:-1] != '' and i[2] != ' ' and i[3][:-1] != ' ':\n",
    "        final_file.append('\\t'.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "168bffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_rus_eng_cleaned.txt', 'w') as file:\n",
    "    for item in final_file:\n",
    "        file.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5a956038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output_rus_eng_cleaned.txt', sep='\\t', header=None, names=['count', 'probability', 'russian', 'english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ccb167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output_rus_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = []\n",
    "for i in file1:\n",
    "    i = i.split('\\t')\n",
    "    eng_word = eng_lemmatize_text(i[2])\n",
    "    rus_word = lemmatize_text(i[3][:-1])\n",
    "    new_i = '\\t'.join([i[0], i[1], eng_word, rus_word])\n",
    "    if eng_word != ' ' and rus_word != ' ' and new_i not in final_file:\n",
    "        final_file.append(new_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97cb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка данных от полупустых строк\n",
    "\n",
    "with open('output-rus-eng-final.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.readlines()\n",
    "\n",
    "c = 0\n",
    "with open('output-rus-eng-final-lem.txt', 'w', encoding='utf-8') as new_file:\n",
    "    for line in text:\n",
    "        if line.split()[0] == '|||' or line.split()[-1:] == ['|||']:\n",
    "            c += 1\n",
    "        else:\n",
    "            new_file.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
